# ğŸ§ IncognitoAI Linux Setup - COMPLETE âœ…

## ğŸ“¦ What Was Created

A complete Linux setup folder with **TWO interfaces** for IncognitoAI:

### Folder Structure
```
linux/
â”œâ”€â”€ ğŸ“„ QUICKSTART.txt              â† START HERE! Quick reference guide
â”œâ”€â”€ ğŸ“„ README.md                   â† Full documentation
â”œâ”€â”€ ğŸ”§ setup.sh                    â† Automated setup script (make executable)
â”œâ”€â”€ â–¶ï¸  start.sh                    â† Launch Streamlit version
â”œâ”€â”€ â–¶ï¸  start_cyberpunk.sh          â† Launch Flask Cyberpunk version
â”œâ”€â”€ ğŸ app_cyberpunk.py            â† Flask application
â”œâ”€â”€ ğŸ“ requirements_flask.txt       â† Flask dependencies
â”œâ”€â”€ ğŸ“ templates/
â”‚   â””â”€â”€ cyberpunk.html             â† Web interface
â””â”€â”€ ğŸ“ static/
    â”œâ”€â”€ cyberpunk.css              â† Neon cyberpunk styling
    â””â”€â”€ cyberpunk.js               â† Interactive features
```

---

## ğŸ¯ TWO INTERFACES AVAILABLE

### 1ï¸âƒ£ **STREAMLIT VERSION** (Original)
- **File:** `start.sh`
- **Port:** `http://localhost:8501`
- **Features:** Simple, clean interface
- **Best for:** Quick testing

### 2ï¸âƒ£ **FLASK CYBERPUNK VERSION** (New!) âœ¨
- **File:** `start_cyberpunk.sh`
- **Port:** `http://localhost:5000`
- **Features:**
  - ğŸŒ Beautiful neon cyberpunk aesthetic
  - ğŸ’¬ Real-time chat interface
  - ğŸ“ Document upload with RAG
  - ğŸ”„ RAG mode toggle
  - ğŸŸ¢ System status indicator
  - ğŸ“± Responsive design

**Recommended:** Use Flask Cyberpunk version for best experience!

---

## ğŸš€ QUICK START (3 Steps)

### Step 1: Initial Setup (ONE TIME)
```bash
cd linux
chmod +x *.sh
./setup.sh
```

This will:
- âœ… Install Python 3 (if needed)
- âœ… Create virtual environment
- âœ… Install dependencies
- âœ… Install Ollama (if needed)
- âœ… Download AI models (5-10 minutes)

### Step 2: Start Ollama (New Terminal)
```bash
ollama serve
```

### Step 3: Launch IncognitoAI (New Terminal)
```bash
cd linux
./start_cyberpunk.sh    # Flask Cyberpunk (recommended!)
# OR
./start.sh              # Streamlit version
```

---

## ğŸ¨ Flask Cyberpunk Features

### Interface Design
- **Dark Theme:** Cyberpunk aesthetic with neon accents
- **Neon Colors:** Pink, cyan, purple, green glows
- **Responsive:** Works on desktop and mobile
- **Animations:** Smooth transitions and effects

### Functionality
- ğŸ’¬ **Real-time Chat:** Type and get instant responses
- ğŸ“ **Upload Documents:** PDF, TXT, Markdown support
- ğŸ§  **RAG Mode:** Chat with your documents
- ğŸ” **Status Monitor:** See if Ollama is running
- ğŸ“Š **System Info:** Model details and features

### User Experience
- Auto-focus on input field
- Enter to send, Shift+Enter for newline
- Typing indicator while waiting for response
- Scroll to latest message
- Clean message history

---

## ğŸ“‹ FILES EXPLAINED

| File | Purpose |
|------|---------|
| `setup.sh` | Automated Linux setup - installs everything |
| `start.sh` | Launches Streamlit interface |
| `start_cyberpunk.sh` | Launches Flask Cyberpunk interface |
| `app_cyberpunk.py` | Flask app with AI logic |
| `cyberpunk.html` | Web page HTML |
| `cyberpunk.css` | Neon cyberpunk styling |
| `cyberpunk.js` | Interactive chat functionality |
| `QUICKSTART.txt` | Quick reference guide |
| `README.md` | Complete documentation |

---

## ğŸ”§ REQUIREMENTS

### System Requirements
- **OS:** Linux (Ubuntu 20.04+, Debian, Fedora, etc.)
- **Python:** 3.8 or higher
- **RAM:** 4GB minimum (8GB recommended)
- **Disk:** 2GB+ free space
- **Internet:** For initial setup only

### Auto-Installed by Setup Script
- Python packages (Flask, LangChain, ChromaDB, etc.)
- Ollama (AI runtime)
- AI models (llama3.2:1b, all-minilm)

---

## ğŸŒŸ KEY FEATURES

### IncognitoAI Linux Setup
âœ… **One-click setup** - Fully automated  
âœ… **Two interfaces** - Streamlit + Flask Cyberpunk  
âœ… **Beautiful design** - Modern cyberpunk aesthetic  
âœ… **Fast performance** - Optimized for Linux  
âœ… **RAG support** - Chat with documents  
âœ… **100% offline** - No data leaves your machine  
âœ… **Easy to use** - Simple shell scripts  
âœ… **Well documented** - Multiple guides included  

---

## âš™ï¸ HOW IT WORKS

### Architecture
```
Your Computer (Linux)
    â†“
Ollama (AI Engine) - Port 11434
    â†“
Flask App (Web Interface) - Port 5000
    â†“
Browser (http://localhost:5000)
```

### Data Flow
1. **User Input** â†’ Web browser sends message
2. **Processing** â†’ Flask app prepares query
3. **AI Response** â†’ Ollama generates response
4. **Display** â†’ Flask sends response to browser
5. **ALL LOCAL** â†’ Nothing leaves your computer

---

## ğŸ“ USAGE EXAMPLES

### Basic Chat
1. Open http://localhost:5000
2. Type your question
3. Press Enter
4. Get instant response

### RAG Mode (Chat with Documents)
1. Click ğŸ“ button
2. Select PDF/TXT/Markdown file
3. Upload completes automatically
4. Enable "RAG" toggle
5. Ask questions about your document

### Models
- **llama3.2:1b** - Fast, efficient AI model
- **all-minilm** - Embedding model for document search

---

## ğŸ› TROUBLESHOOTING

### Setup Issues

**Q: "ollama: command not found"**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Q: "Python not found"**
```bash
sudo apt-get update
sudo apt-get install python3 python3-pip
```

**Q: "Permission denied on script"**
```bash
chmod +x setup.sh start.sh start_cyberpunk.sh
```

### Runtime Issues

**Q: "Connection refused" on port 5000**
```bash
# Port already in use - edit app_cyberpunk.py:
# Change: app.run(debug=True, host='0.0.0.0', port=5000)
# To:     app.run(debug=True, host='0.0.0.0', port=5001)
```

**Q: "Ollama offline" warning**
- Make sure to run `ollama serve` in another terminal

**Q: "Models not loading"**
```bash
ollama pull llama3.2:1b
ollama pull all-minilm
```

---

## ğŸ”’ PRIVACY & SECURITY

âœ… **100% Offline** - No data sent anywhere  
âœ… **No Tracking** - Zero telemetry  
âœ… **No Ads** - Clean interface  
âœ… **Open Source** - MIT License  
âœ… **Your Data** - Stored only on your machine  
âœ… **No Account** - No login required  

---

## ğŸ“š DOCUMENTATION

- **[QUICKSTART.txt](QUICKSTART.txt)** - Quick reference (read first!)
- **[README.md](README.md)** - Complete guide
- **[../README.md](../README.md)** - Main project docs
- Ollama: https://ollama.ai
- Flask: https://flask.palletsprojects.com

---

## ğŸ’¡ TIPS & TRICKS

1. **Use both interfaces** - Try Streamlit and Cyberpunk
2. **Upload documents** - Test RAG with your PDFs
3. **Clear cache** - Delete `.chroma_db` folder to reset
4. **Change theme** - Edit `static/cyberpunk.css`
5. **Different models** - Change `MODEL_NAME` in `app_cyberpunk.py`

---

## ğŸ“ LEARNING PATH

1. Run setup script
2. Open QUICKSTART.txt
3. Try Flask Cyberpunk version
4. Upload a test document
5. Explore RAG mode
6. Read full README.md
7. Customize and extend

---

## ğŸ“ SUPPORT

Having issues? Check:
1. QUICKSTART.txt (this file) - Common issues
2. README.md - Detailed guide
3. ../README.md - Main documentation
4. Terminal error messages - Usually very helpful

---

## ğŸ‰ YOU'RE ALL SET!

Your Linux IncognitoAI setup is complete and ready to use.

### Next Steps:
1. Run `./setup.sh` in the linux folder
2. Start Ollama in another terminal
3. Launch the Cyberpunk interface
4. Start chatting!

**Enjoy your private, offline AI assistant!** ğŸŒâœ¨

---

*IncognitoAI - Made with â¤ï¸ for privacy-conscious users*
*MIT License - Open Source*
